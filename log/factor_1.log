This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=0), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7fa79b01c940>]
1.6689300537109375e-06
ft_net(
  (model): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (class_0): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_1): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_2): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_3): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_4): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_5): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_6): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_7): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_8): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_9): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_10): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_11): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_12): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_13): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_14): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_15): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_16): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_17): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_18): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_19): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_20): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_21): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_22): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_23): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_24): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_25): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_26): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_27): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_28): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_29): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_30): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=256, bias=True)
      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (classifier): Sequential(
      (0): Linear(in_features=256, out_features=751, bias=True)
    )
  )
)
Epoch 0/59
----------
train Loss: 5.9737 Acc: 0.0757
val Loss: 6.8013 Acc: 0.0015
Training complete in 1m 28s

Epoch 1/59
----------
train Loss: 4.1959 Acc: 0.2668
val Loss: 7.1880 Acc: 0.0021
Training complete in 2m 50s

Epoch 2/59
----------
train Loss: 2.9031 Acc: 0.4893
val Loss: 7.4311 Acc: 0.0030
Training complete in 4m 13s

Epoch 3/59
----------
train Loss: 1.9867 Acc: 0.6770
val Loss: 8.0098 Acc: 0.0021
Training complete in 5m 36s

Epoch 4/59
----------
train Loss: 1.3396 Acc: 0.8001
val Loss: 8.2228 Acc: 0.0018
Training complete in 6m 59s

Epoch 5/59
----------
train Loss: 0.9147 Acc: 0.8736
val Loss: 8.5182 Acc: 0.0027
Training complete in 8m 23s

Epoch 6/59
----------
train Loss: 0.6265 Acc: 0.9245
val Loss: 8.7446 Acc: 0.0021
Training complete in 9m 46s

Epoch 7/59
----------
train Loss: 0.4314 Acc: 0.9555
val Loss: 8.9752 Acc: 0.0018
Training complete in 11m 7s

Epoch 8/59
----------
train Loss: 0.3103 Acc: 0.9739
val Loss: 9.0715 Acc: 0.0024
Training complete in 12m 29s

Epoch 9/59
----------
train Loss: 0.2252 Acc: 0.9843
val Loss: 9.1465 Acc: 0.0018
Training complete in 13m 53s

Epoch 10/59
----------
train Loss: 0.1695 Acc: 0.9898
val Loss: 9.2043 Acc: 0.0027
Training complete in 15m 16s

Epoch 11/59
----------
train Loss: 0.1308 Acc: 0.9949
val Loss: 9.2425 Acc: 0.0027
Training complete in 16m 39s

Epoch 12/59
----------
train Loss: 0.1065 Acc: 0.9964
val Loss: 9.2929 Acc: 0.0021
Training complete in 18m 1s

Epoch 13/59
----------
train Loss: 0.0869 Acc: 0.9981
val Loss: 9.4047 Acc: 0.0027
Training complete in 19m 23s

Epoch 14/59
----------
train Loss: 0.0754 Acc: 0.9988
val Loss: 9.3753 Acc: 0.0027
Training complete in 20m 46s

Epoch 15/59
----------
train Loss: 0.0651 Acc: 0.9988
val Loss: 9.3859 Acc: 0.0021
Training complete in 22m 8s

Epoch 16/59
----------
train Loss: 0.0572 Acc: 0.9991
val Loss: 9.4921 Acc: 0.0021
Training complete in 23m 31s

Epoch 17/59
----------
train Loss: 0.0520 Acc: 0.9992
val Loss: 9.4507 Acc: 0.0024
Training complete in 24m 53s

Epoch 18/59
----------
train Loss: 0.0479 Acc: 0.9994
val Loss: 9.5488 Acc: 0.0021
Training complete in 26m 15s

Epoch 19/59
----------
train Loss: 0.0443 Acc: 0.9994
val Loss: 9.4527 Acc: 0.0021
Training complete in 27m 40s

Epoch 20/59
----------
train Loss: 0.0418 Acc: 0.9994
val Loss: 9.4506 Acc: 0.0024
Training complete in 29m 2s

Epoch 21/59
----------
train Loss: 0.0397 Acc: 0.9994
val Loss: 9.4186 Acc: 0.0015
Training complete in 30m 25s

Epoch 22/59
----------
train Loss: 0.0373 Acc: 0.9994
val Loss: 9.5004 Acc: 0.0021
Training complete in 31m 48s

Epoch 23/59
----------
train Loss: 0.0363 Acc: 0.9994
val Loss: 9.4941 Acc: 0.0021
Training complete in 33m 11s

Epoch 24/59
----------
train Loss: 0.0343 Acc: 0.9994
val Loss: 9.5104 Acc: 0.0015
Training complete in 34m 33s

Epoch 25/59
----------
train Loss: 0.0332 Acc: 0.9994
val Loss: 9.5379 Acc: 0.0015
Training complete in 35m 56s

Epoch 26/59
----------
train Loss: 0.0318 Acc: 0.9994
val Loss: 9.4719 Acc: 0.0012
Training complete in 37m 18s

Epoch 27/59
----------
train Loss: 0.0306 Acc: 0.9994
val Loss: 9.4450 Acc: 0.0024
Training complete in 38m 41s

Epoch 28/59
----------
train Loss: 0.0294 Acc: 0.9994
val Loss: 9.4931 Acc: 0.0018
Training complete in 40m 3s

Epoch 29/59
----------
train Loss: 0.0289 Acc: 0.9994
val Loss: 9.3746 Acc: 0.0015
Training complete in 41m 28s

Epoch 30/59
----------
train Loss: 0.0281 Acc: 0.9994
val Loss: 9.4181 Acc: 0.0018
Training complete in 42m 50s

Epoch 31/59
----------
train Loss: 0.0272 Acc: 0.9994
val Loss: 9.4775 Acc: 0.0018
Training complete in 44m 13s

Epoch 32/59
----------
train Loss: 0.0267 Acc: 0.9994
val Loss: 9.4805 Acc: 0.0015
Training complete in 45m 35s

Epoch 33/59
----------
train Loss: 0.0261 Acc: 0.9994
val Loss: 9.3820 Acc: 0.0015
Training complete in 46m 58s

Epoch 34/59
----------
train Loss: 0.0258 Acc: 0.9994
val Loss: 9.4085 Acc: 0.0018
Training complete in 48m 21s

Epoch 35/59
----------
train Loss: 0.0247 Acc: 0.9994
val Loss: 9.3969 Acc: 0.0012
Training complete in 49m 43s

Epoch 36/59
----------
train Loss: 0.0241 Acc: 0.9994
val Loss: 9.3730 Acc: 0.0006
Training complete in 51m 6s

Epoch 37/59
----------
train Loss: 0.0243 Acc: 0.9994
val Loss: 9.3390 Acc: 0.0018
Training complete in 52m 28s

Epoch 38/59
----------
train Loss: 0.0241 Acc: 0.9994
val Loss: 9.3497 Acc: 0.0015
Training complete in 53m 51s

Epoch 39/59
----------
train Loss: 0.0239 Acc: 0.9994
val Loss: 9.3441 Acc: 0.0012
Training complete in 55m 14s

Epoch 40/59
----------
train Loss: 0.0225 Acc: 0.9994
val Loss: 9.3221 Acc: 0.0015
Training complete in 56m 37s

Epoch 41/59
----------
train Loss: 0.0217 Acc: 0.9994
val Loss: 9.3243 Acc: 0.0015
Training complete in 57m 60s

Epoch 42/59
----------
train Loss: 0.0215 Acc: 0.9994
val Loss: 9.3056 Acc: 0.0015
Training complete in 59m 23s

Epoch 43/59
----------
train Loss: 0.0213 Acc: 0.9994
val Loss: 9.3201 Acc: 0.0012
Training complete in 60m 45s

Epoch 44/59
----------
train Loss: 0.0210 Acc: 0.9994
val Loss: 9.3234 Acc: 0.0018
Training complete in 62m 9s

Epoch 45/59
----------
train Loss: 0.0211 Acc: 0.9994
val Loss: 9.3199 Acc: 0.0015
Training complete in 63m 32s

Epoch 46/59
----------
train Loss: 0.0206 Acc: 0.9994
val Loss: 9.3011 Acc: 0.0015
Training complete in 64m 55s

Epoch 47/59
----------
train Loss: 0.0211 Acc: 0.9994
val Loss: 9.3406 Acc: 0.0015
Training complete in 66m 17s

Epoch 48/59
----------
train Loss: 0.0205 Acc: 0.9994
val Loss: 9.2945 Acc: 0.0009
Training complete in 67m 40s

Epoch 49/59
----------
train Loss: 0.0206 Acc: 0.9994
val Loss: 9.2724 Acc: 0.0012
Training complete in 69m 4s

Epoch 50/59
----------
train Loss: 0.0208 Acc: 0.9994
val Loss: 9.3201 Acc: 0.0015
Training complete in 70m 27s

Epoch 51/59
----------
train Loss: 0.0206 Acc: 0.9994
val Loss: 9.2935 Acc: 0.0015
Training complete in 71m 50s

Epoch 52/59
----------
train Loss: 0.0203 Acc: 0.9994
val Loss: 9.2977 Acc: 0.0015
Training complete in 73m 13s

Epoch 53/59
----------
train Loss: 0.0210 Acc: 0.9994
val Loss: 9.3001 Acc: 0.0012
Training complete in 74m 36s

Epoch 54/59
----------
train Loss: 0.0206 Acc: 0.9994
val Loss: 9.3019 Acc: 0.0015
Training complete in 75m 59s

Epoch 55/59
----------
train Loss: 0.0205 Acc: 0.9994
val Loss: 9.2734 Acc: 0.0015
Training complete in 77m 21s

Epoch 56/59
----------
train Loss: 0.0208 Acc: 0.9994
val Loss: 9.2446 Acc: 0.0015
Training complete in 78m 44s

Epoch 57/59
----------
train Loss: 0.0199 Acc: 0.9994
val Loss: 9.2770 Acc: 0.0012
Training complete in 80m 8s

Epoch 58/59
----------
train Loss: 0.0205 Acc: 0.9994
val Loss: 9.2909 Acc: 0.0012
Training complete in 81m 30s

Epoch 59/59
----------
train Loss: 0.0207 Acc: 0.9994
val Loss: 9.2708 Acc: 0.0012
Training complete in 82m 55s

Training complete in 82m 55s
This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0
-------test-----------
256
512
768
1024
1280
1536
1792
2048
2304
2560
2816
3072
3328
3584
3840
4096
4352
4608
4864
5120
5376
5632
5888
6144
6400
6656
6912
7168
7424
7680
7936
8192
8448
8704
8960
9216
9472
9728
9984
10240
10496
10752
11008
11264
11520
11776
12032
12288
12544
12800
13056
13312
13568
13824
14080
14336
14592
14848
15104
15360
15616
15872
16128
16384
16640
16896
17152
17408
17664
17920
18176
18432
18688
18944
19200
19456
19712
19732
256
512
768
1024
1280
1536
1792
2048
2304
2560
2816
3072
3328
3368
torch.Size([3368, 256])
Rank@1:0.861045 Rank@5:0.941508 Rank@10:0.961698 mAP:0.683093
