This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=0), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7fb3b08ad9b0>]
1.6689300537109375e-06
ft_net(
  (model): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (class_0): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_1): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_2): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_3): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_4): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_5): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_6): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_7): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_8): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_9): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_10): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_11): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_12): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_13): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_14): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_15): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_16): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_17): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_18): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_19): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_20): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_21): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_22): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_23): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_24): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_25): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_26): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_27): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_28): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_29): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_30): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=256, bias=True)
      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (classifier): Sequential(
      (0): Linear(in_features=256, out_features=751, bias=True)
    )
  )
)
Epoch 0/59
----------
train Loss: 5.9877 Acc: 0.0737
val Loss: 6.8311 Acc: 0.0003
Training complete in 1m 30s

Epoch 1/59
----------
train Loss: 4.2209 Acc: 0.2586
val Loss: 7.1685 Acc: 0.0027
Training complete in 2m 53s

Epoch 2/59
----------
train Loss: 2.9283 Acc: 0.4902
val Loss: 7.5648 Acc: 0.0018
Training complete in 4m 16s

Epoch 3/59
----------
train Loss: 1.9958 Acc: 0.6753
val Loss: 7.9499 Acc: 0.0015
Training complete in 5m 40s

Epoch 4/59
----------
train Loss: 1.3559 Acc: 0.7968
val Loss: 8.2237 Acc: 0.0024
Training complete in 7m 5s

Epoch 5/59
----------
train Loss: 0.9194 Acc: 0.8731
val Loss: 8.5638 Acc: 0.0015
Training complete in 8m 28s

Epoch 6/59
----------
train Loss: 0.6230 Acc: 0.9242
val Loss: 8.6835 Acc: 0.0018
Training complete in 9m 52s

Epoch 7/59
----------
train Loss: 0.4315 Acc: 0.9557
val Loss: 8.9412 Acc: 0.0024
Training complete in 11m 16s

Epoch 8/59
----------
train Loss: 0.3078 Acc: 0.9745
val Loss: 9.1206 Acc: 0.0018
Training complete in 12m 40s

Epoch 9/59
----------
train Loss: 0.2208 Acc: 0.9848
val Loss: 9.1504 Acc: 0.0018
Training complete in 14m 12s

Epoch 10/59
----------
train Loss: 0.1643 Acc: 0.9913
val Loss: 9.2994 Acc: 0.0021
Training complete in 15m 36s

Epoch 11/59
----------
train Loss: 0.1314 Acc: 0.9945
val Loss: 9.3409 Acc: 0.0012
Training complete in 17m 1s

Epoch 12/59
----------
train Loss: 0.1019 Acc: 0.9968
val Loss: 9.2985 Acc: 0.0018
Training complete in 18m 27s

Epoch 13/59
----------
train Loss: 0.0832 Acc: 0.9979
val Loss: 9.4550 Acc: 0.0018
Training complete in 19m 52s

Epoch 14/59
----------
train Loss: 0.0709 Acc: 0.9986
val Loss: 9.4493 Acc: 0.0018
Training complete in 21m 16s

Epoch 15/59
----------
train Loss: 0.0612 Acc: 0.9990
val Loss: 9.5309 Acc: 0.0015
Training complete in 22m 40s

Epoch 16/59
----------
train Loss: 0.0561 Acc: 0.9991
val Loss: 9.5187 Acc: 0.0015
Training complete in 24m 4s

Epoch 17/59
----------
train Loss: 0.0500 Acc: 0.9990
val Loss: 9.5551 Acc: 0.0018
Training complete in 25m 27s

Epoch 18/59
----------
train Loss: 0.0453 Acc: 0.9993
val Loss: 9.5512 Acc: 0.0015
Training complete in 26m 51s

Epoch 19/59
----------
train Loss: 0.0414 Acc: 0.9992
val Loss: 9.5200 Acc: 0.0015
Training complete in 28m 15s

Epoch 20/59
----------
train Loss: 0.0378 Acc: 0.9994
val Loss: 9.5548 Acc: 0.0015
Training complete in 29m 40s

Epoch 21/59
----------
train Loss: 0.0366 Acc: 0.9994
val Loss: 9.5430 Acc: 0.0018
Training complete in 31m 4s

Epoch 22/59
----------
train Loss: 0.0345 Acc: 0.9994
val Loss: 9.5704 Acc: 0.0012
Training complete in 32m 28s

Epoch 23/59
----------
train Loss: 0.0338 Acc: 0.9994
val Loss: 9.5512 Acc: 0.0015
Training complete in 33m 52s

Epoch 24/59
----------
train Loss: 0.0315 Acc: 0.9994
val Loss: 9.6717 Acc: 0.0015
Training complete in 35m 16s

Epoch 25/59
----------
train Loss: 0.0308 Acc: 0.9994
val Loss: 9.5945 Acc: 0.0015
Training complete in 36m 40s

Epoch 26/59
----------
train Loss: 0.0299 Acc: 0.9994
val Loss: 9.4910 Acc: 0.0018
Training complete in 38m 4s

Epoch 27/59
----------
train Loss: 0.0292 Acc: 0.9994
val Loss: 9.5359 Acc: 0.0018
Training complete in 39m 27s

Epoch 28/59
----------
train Loss: 0.0282 Acc: 0.9994
val Loss: 9.5608 Acc: 0.0015
Training complete in 40m 51s

Epoch 29/59
----------
train Loss: 0.0276 Acc: 0.9994
val Loss: 9.5501 Acc: 0.0015
Training complete in 42m 17s

Epoch 30/59
----------
train Loss: 0.0271 Acc: 0.9994
val Loss: 9.4665 Acc: 0.0015
Training complete in 43m 41s

Epoch 31/59
----------
train Loss: 0.0265 Acc: 0.9994
val Loss: 9.4832 Acc: 0.0015
Training complete in 45m 5s

Epoch 32/59
----------
train Loss: 0.0258 Acc: 0.9994
val Loss: 9.4686 Acc: 0.0018
Training complete in 46m 29s

Epoch 33/59
----------
train Loss: 0.0258 Acc: 0.9994
val Loss: 9.4512 Acc: 0.0015
Training complete in 47m 53s

Epoch 34/59
----------
train Loss: 0.0252 Acc: 0.9994
val Loss: 9.4150 Acc: 0.0012
Training complete in 49m 17s

Epoch 35/59
----------
train Loss: 0.0245 Acc: 0.9994
val Loss: 9.4168 Acc: 0.0015
Training complete in 50m 41s

Epoch 36/59
----------
train Loss: 0.0245 Acc: 0.9994
val Loss: 9.3694 Acc: 0.0018
Training complete in 52m 5s

Epoch 37/59
----------
train Loss: 0.0233 Acc: 0.9994
val Loss: 9.4232 Acc: 0.0024
Training complete in 53m 29s

Epoch 38/59
----------
train Loss: 0.0234 Acc: 0.9994
val Loss: 9.4457 Acc: 0.0021
Training complete in 54m 53s

Epoch 39/59
----------
train Loss: 0.0234 Acc: 0.9994
val Loss: 9.3490 Acc: 0.0021
Training complete in 56m 19s

Epoch 40/59
----------
train Loss: 0.0226 Acc: 0.9994
val Loss: 9.3471 Acc: 0.0018
Training complete in 57m 43s

Epoch 41/59
----------
train Loss: 0.0225 Acc: 0.9994
val Loss: 9.3685 Acc: 0.0018
Training complete in 59m 8s

Epoch 42/59
----------
train Loss: 0.0217 Acc: 0.9994
val Loss: 9.3251 Acc: 0.0015
Training complete in 60m 33s

Epoch 43/59
----------
train Loss: 0.0215 Acc: 0.9994
val Loss: 9.3536 Acc: 0.0018
Training complete in 61m 58s

Epoch 44/59
----------
train Loss: 0.0216 Acc: 0.9994
val Loss: 9.3408 Acc: 0.0018
Training complete in 63m 23s

Epoch 45/59
----------
train Loss: 0.0212 Acc: 0.9994
val Loss: 9.3593 Acc: 0.0018
Training complete in 64m 48s

Epoch 46/59
----------
train Loss: 0.0214 Acc: 0.9994
val Loss: 9.3320 Acc: 0.0018
Training complete in 66m 12s

Epoch 47/59
----------
train Loss: 0.0211 Acc: 0.9994
val Loss: 9.3579 Acc: 0.0018
Training complete in 67m 36s

Epoch 48/59
----------
train Loss: 0.0211 Acc: 0.9994
val Loss: 9.3337 Acc: 0.0018
Training complete in 69m 0s

Epoch 49/59
----------
train Loss: 0.0212 Acc: 0.9994
val Loss: 9.3570 Acc: 0.0018
Training complete in 70m 26s

Epoch 50/59
----------
train Loss: 0.0213 Acc: 0.9994
val Loss: 9.2864 Acc: 0.0015
Training complete in 71m 51s

Epoch 51/59
----------
train Loss: 0.0208 Acc: 0.9994
val Loss: 9.3304 Acc: 0.0018
Training complete in 73m 16s

Epoch 52/59
----------
train Loss: 0.0211 Acc: 0.9994
val Loss: 9.3617 Acc: 0.0018
Training complete in 74m 39s

Epoch 53/59
----------
train Loss: 0.0208 Acc: 0.9994
val Loss: 9.3541 Acc: 0.0015
Training complete in 76m 3s

Epoch 54/59
----------
train Loss: 0.0209 Acc: 0.9994
val Loss: 9.3122 Acc: 0.0018
Training complete in 77m 27s

Epoch 55/59
----------
train Loss: 0.0210 Acc: 0.9994
val Loss: 9.3262 Acc: 0.0018
Training complete in 78m 51s

Epoch 56/59
----------
train Loss: 0.0209 Acc: 0.9994
val Loss: 9.3423 Acc: 0.0018
Training complete in 80m 15s

Epoch 57/59
----------
train Loss: 0.0205 Acc: 0.9994
val Loss: 9.3028 Acc: 0.0018
Training complete in 81m 39s

Epoch 58/59
----------
train Loss: 0.0208 Acc: 0.9994
val Loss: 9.3310 Acc: 0.0015
Training complete in 83m 3s

Epoch 59/59
----------
train Loss: 0.0207 Acc: 0.9994
val Loss: 9.3077 Acc: 0.0018
Training complete in 84m 31s

Training complete in 84m 31s
This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0
-------test-----------
256
512
768
1024
1280
1536
1792
2048
2304
2560
2816
3072
3328
3584
3840
4096
4352
4608
4864
5120
5376
5632
5888
6144
6400
6656
6912
7168
7424
7680
7936
8192
8448
8704
8960
9216
9472
9728
9984
10240
10496
10752
11008
11264
11520
11776
12032
12288
12544
12800
13056
13312
13568
13824
14080
14336
14592
14848
15104
15360
15616
15872
16128
16384
16640
16896
17152
17408
17664
17920
18176
18432
18688
18944
19200
19456
19712
19732
256
512
768
1024
1280
1536
1792
2048
2304
2560
2816
3072
3328
3368
torch.Size([3368, 256])
Rank@1:0.858670 Rank@5:0.947447 Rank@10:0.960808 mAP:0.683117
