This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=0), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7f3485f339b0>]
1.6689300537109375e-06
ft_net(
  (model): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (class_0): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_1): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_2): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_3): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_4): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_5): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_6): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_7): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_8): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_9): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_10): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_11): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_12): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_13): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_14): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_15): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_16): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_17): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_18): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_19): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_20): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_21): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_22): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_23): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_24): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_25): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_26): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_27): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_28): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_29): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=128, bias=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (class_30): ClassBlock(
    (dropout): Dropout(p=0.5)
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=256, bias=True)
      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (classifier): Sequential(
      (0): Linear(in_features=256, out_features=751, bias=True)
    )
  )
)
Epoch 0/59
----------
train Loss: 5.9962 Acc: 0.0669
val Loss: 6.7727 Acc: 0.0012
Training complete in 1m 24s

Epoch 1/59
----------
train Loss: 4.2313 Acc: 0.2631
val Loss: 7.1325 Acc: 0.0036
Training complete in 2m 44s

Epoch 2/59
----------
train Loss: 2.9220 Acc: 0.4897
val Loss: 7.4783 Acc: 0.0024
Training complete in 4m 4s

Epoch 3/59
----------
train Loss: 1.9999 Acc: 0.6729
val Loss: 7.8752 Acc: 0.0018
Training complete in 5m 24s

Epoch 4/59
----------
train Loss: 1.3511 Acc: 0.7968
val Loss: 8.1950 Acc: 0.0021
Training complete in 6m 44s

Epoch 5/59
----------
train Loss: 0.9156 Acc: 0.8735
val Loss: 8.5209 Acc: 0.0024
Training complete in 8m 5s

Epoch 6/59
----------
train Loss: 0.6232 Acc: 0.9246
val Loss: 8.8422 Acc: 0.0018
Training complete in 9m 25s

Epoch 7/59
----------
train Loss: 0.4335 Acc: 0.9560
val Loss: 8.8814 Acc: 0.0015
Training complete in 10m 45s

Epoch 8/59
----------
train Loss: 0.3046 Acc: 0.9739
val Loss: 8.9361 Acc: 0.0012
Training complete in 12m 6s

Epoch 9/59
----------
train Loss: 0.2203 Acc: 0.9845
val Loss: 9.1244 Acc: 0.0030
Training complete in 13m 28s

Epoch 10/59
----------
train Loss: 0.1701 Acc: 0.9903
val Loss: 9.2915 Acc: 0.0024
Training complete in 14m 48s

Epoch 11/59
----------
train Loss: 0.1296 Acc: 0.9937
val Loss: 9.2997 Acc: 0.0021
Training complete in 16m 9s

Epoch 12/59
----------
train Loss: 0.1019 Acc: 0.9973
val Loss: 9.3328 Acc: 0.0021
Training complete in 17m 30s

Epoch 13/59
----------
train Loss: 0.0857 Acc: 0.9978
val Loss: 9.4029 Acc: 0.0018
Training complete in 18m 50s

Epoch 14/59
----------
train Loss: 0.0710 Acc: 0.9988
val Loss: 9.3825 Acc: 0.0024
Training complete in 20m 12s

Epoch 15/59
----------
train Loss: 0.0611 Acc: 0.9991
val Loss: 9.5624 Acc: 0.0021
Training complete in 21m 33s

Epoch 16/59
----------
train Loss: 0.0545 Acc: 0.9994
val Loss: 9.5204 Acc: 0.0036
Training complete in 22m 53s

Epoch 17/59
----------
train Loss: 0.0489 Acc: 0.9993
val Loss: 9.4409 Acc: 0.0021
Training complete in 24m 13s

Epoch 18/59
----------
train Loss: 0.0443 Acc: 0.9994
val Loss: 9.5131 Acc: 0.0027
Training complete in 25m 34s

Epoch 19/59
----------
train Loss: 0.0403 Acc: 0.9994
val Loss: 9.5178 Acc: 0.0021
Training complete in 26m 55s

Epoch 20/59
----------
train Loss: 0.0383 Acc: 0.9994
val Loss: 9.5065 Acc: 0.0021
Training complete in 28m 16s

Epoch 21/59
----------
train Loss: 0.0358 Acc: 0.9994
val Loss: 9.4929 Acc: 0.0021
Training complete in 29m 37s

Epoch 22/59
----------
train Loss: 0.0337 Acc: 0.9994
val Loss: 9.5482 Acc: 0.0018
Training complete in 30m 57s

Epoch 23/59
----------
train Loss: 0.0323 Acc: 0.9994
val Loss: 9.5087 Acc: 0.0024
Training complete in 32m 18s

Epoch 24/59
----------
train Loss: 0.0317 Acc: 0.9994
val Loss: 9.4997 Acc: 0.0024
Training complete in 33m 38s

Epoch 25/59
----------
train Loss: 0.0307 Acc: 0.9994
val Loss: 9.5045 Acc: 0.0018
Training complete in 34m 59s

Epoch 26/59
----------
train Loss: 0.0298 Acc: 0.9994
val Loss: 9.5759 Acc: 0.0024
Training complete in 36m 20s

Epoch 27/59
----------
train Loss: 0.0285 Acc: 0.9994
val Loss: 9.4469 Acc: 0.0015
Training complete in 37m 40s

Epoch 28/59
----------
train Loss: 0.0280 Acc: 0.9994
val Loss: 9.3902 Acc: 0.0021
Training complete in 39m 1s

Epoch 29/59
----------
train Loss: 0.0269 Acc: 0.9994
val Loss: 9.4394 Acc: 0.0018
Training complete in 40m 22s

Epoch 30/59
----------
train Loss: 0.0261 Acc: 0.9994
val Loss: 9.4892 Acc: 0.0024
Training complete in 41m 42s

Epoch 31/59
----------
train Loss: 0.0257 Acc: 0.9994
val Loss: 9.4645 Acc: 0.0024
Training complete in 43m 3s

Epoch 32/59
----------
train Loss: 0.0256 Acc: 0.9994
val Loss: 9.3509 Acc: 0.0021
Training complete in 44m 23s

Epoch 33/59
----------
train Loss: 0.0247 Acc: 0.9994
val Loss: 9.3512 Acc: 0.0027
Training complete in 45m 44s

Epoch 34/59
----------
train Loss: 0.0243 Acc: 0.9994
val Loss: 9.3601 Acc: 0.0021
Training complete in 47m 5s

Epoch 35/59
----------
train Loss: 0.0242 Acc: 0.9994
val Loss: 9.3760 Acc: 0.0021
Training complete in 48m 25s

Epoch 36/59
----------
train Loss: 0.0247 Acc: 0.9994
val Loss: 9.3729 Acc: 0.0024
Training complete in 49m 46s

Epoch 37/59
----------
train Loss: 0.0234 Acc: 0.9994
val Loss: 9.3662 Acc: 0.0021
Training complete in 51m 7s

Epoch 38/59
----------
train Loss: 0.0235 Acc: 0.9994
val Loss: 9.2855 Acc: 0.0024
Training complete in 52m 28s

Epoch 39/59
----------
train Loss: 0.0228 Acc: 0.9994
val Loss: 9.3041 Acc: 0.0021
Training complete in 53m 51s

Epoch 40/59
----------
train Loss: 0.0223 Acc: 0.9994
val Loss: 9.2988 Acc: 0.0021
Training complete in 55m 12s

Epoch 41/59
----------
train Loss: 0.0210 Acc: 0.9994
val Loss: 9.2768 Acc: 0.0021
Training complete in 56m 33s

Epoch 42/59
----------
train Loss: 0.0211 Acc: 0.9994
val Loss: 9.2923 Acc: 0.0021
Training complete in 57m 53s

Epoch 43/59
----------
train Loss: 0.0217 Acc: 0.9994
val Loss: 9.3089 Acc: 0.0021
Training complete in 59m 14s

Epoch 44/59
----------
train Loss: 0.0207 Acc: 0.9994
val Loss: 9.3055 Acc: 0.0021
Training complete in 60m 35s

Epoch 45/59
----------
train Loss: 0.0211 Acc: 0.9994
val Loss: 9.2469 Acc: 0.0021
Training complete in 61m 55s

Epoch 46/59
----------
train Loss: 0.0212 Acc: 0.9994
val Loss: 9.3131 Acc: 0.0024
Training complete in 63m 16s

Epoch 47/59
----------
train Loss: 0.0209 Acc: 0.9994
val Loss: 9.2839 Acc: 0.0021
Training complete in 64m 36s

Epoch 48/59
----------
train Loss: 0.0210 Acc: 0.9994
val Loss: 9.2870 Acc: 0.0021
Training complete in 65m 57s

Epoch 49/59
----------
train Loss: 0.0211 Acc: 0.9994
val Loss: 9.2962 Acc: 0.0021
Training complete in 67m 19s

Epoch 50/59
----------
train Loss: 0.0210 Acc: 0.9994
val Loss: 9.2886 Acc: 0.0021
Training complete in 68m 40s

Epoch 51/59
----------
train Loss: 0.0212 Acc: 0.9994
val Loss: 9.2837 Acc: 0.0021
Training complete in 70m 1s

Epoch 52/59
----------
train Loss: 0.0207 Acc: 0.9994
val Loss: 9.2562 Acc: 0.0021
Training complete in 71m 21s

Epoch 53/59
----------
train Loss: 0.0205 Acc: 0.9994
val Loss: 9.2640 Acc: 0.0021
Training complete in 72m 42s

Epoch 54/59
----------
train Loss: 0.0205 Acc: 0.9994
val Loss: 9.2873 Acc: 0.0024
Training complete in 74m 4s

Epoch 55/59
----------
train Loss: 0.0209 Acc: 0.9994
val Loss: 9.2726 Acc: 0.0024
Training complete in 75m 27s

Epoch 56/59
----------
train Loss: 0.0208 Acc: 0.9994
val Loss: 9.2790 Acc: 0.0024
Training complete in 76m 49s

Epoch 57/59
----------
train Loss: 0.0210 Acc: 0.9994
val Loss: 9.2775 Acc: 0.0024
Training complete in 78m 9s

Epoch 58/59
----------
train Loss: 0.0209 Acc: 0.9994
val Loss: 9.2715 Acc: 0.0021
Training complete in 79m 29s

Epoch 59/59
----------
train Loss: 0.0207 Acc: 0.9994
val Loss: 9.2298 Acc: 0.0021
Training complete in 80m 53s

Training complete in 80m 53s
This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0
-------test-----------
256
512
768
1024
1280
1536
1792
2048
2304
2560
2816
3072
3328
3584
3840
4096
4352
4608
4864
5120
5376
5632
5888
6144
6400
6656
6912
7168
7424
7680
7936
8192
8448
8704
8960
9216
9472
9728
9984
10240
10496
10752
11008
11264
11520
11776
12032
12288
12544
12800
13056
13312
13568
13824
14080
14336
14592
14848
15104
15360
15616
15872
16128
16384
16640
16896
17152
17408
17664
17920
18176
18432
18688
18944
19200
19456
19712
19732
256
512
768
1024
1280
1536
1792
2048
2304
2560
2816
3072
3328
3368
torch.Size([3368, 256])
Rank@1:0.861045 Rank@5:0.940915 Rank@10:0.958135 mAP:0.678160
