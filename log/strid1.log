This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]
1.7163031101226807
ft_net(
  (model): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=751, bias=True)
    )
  )
)
Epoch 0/59
----------
train Loss: 3.2061 Acc: 0.3941
val Loss: 1.0974 Acc: 0.6871
Training complete in 2m 9s

Epoch 1/59
----------
train Loss: 0.6332 Acc: 0.8513
val Loss: 0.3010 Acc: 0.8589
Training complete in 4m 18s

Epoch 2/59
----------
train Loss: 0.2565 Acc: 0.9392
val Loss: 0.0836 Acc: 0.9201
Training complete in 6m 27s

Epoch 3/59
----------
train Loss: 0.1288 Acc: 0.9746
val Loss: 0.0388 Acc: 0.9321
Training complete in 8m 37s

Epoch 4/59
----------
train Loss: 0.0696 Acc: 0.9889
val Loss: 0.0242 Acc: 0.9321
Training complete in 10m 46s

Epoch 5/59
----------
train Loss: 0.0559 Acc: 0.9940
val Loss: 0.0153 Acc: 0.9374
Training complete in 12m 60s

Epoch 6/59
----------
train Loss: 0.0433 Acc: 0.9957
val Loss: 0.0130 Acc: 0.9374
Training complete in 15m 19s

Epoch 7/59
----------
train Loss: 0.0351 Acc: 0.9972
val Loss: 0.0103 Acc: 0.9374
Training complete in 17m 37s

Epoch 8/59
----------
train Loss: 0.0298 Acc: 0.9984
val Loss: 0.0101 Acc: 0.9374
Training complete in 19m 55s

Epoch 9/59
----------
train Loss: 0.0284 Acc: 0.9990
val Loss: 0.0099 Acc: 0.9374
Training complete in 22m 10s

Epoch 10/59
----------
train Loss: 0.0276 Acc: 0.9991
val Loss: 0.0108 Acc: 0.9374
Training complete in 24m 27s

Epoch 11/59
----------
train Loss: 0.0281 Acc: 0.9993
val Loss: 0.0110 Acc: 0.9374
Training complete in 26m 45s

Epoch 12/59
----------
train Loss: 0.0279 Acc: 0.9994
val Loss: 0.0103 Acc: 0.9374
Training complete in 29m 2s

Epoch 13/59
----------
train Loss: 0.0288 Acc: 0.9993
val Loss: 0.0109 Acc: 0.9374
Training complete in 31m 20s

Epoch 14/59
----------
train Loss: 0.0287 Acc: 0.9994
val Loss: 0.0112 Acc: 0.9374
Training complete in 33m 40s

Epoch 15/59
----------
train Loss: 0.0286 Acc: 0.9993
val Loss: 0.0120 Acc: 0.9374
Training complete in 35m 52s

Epoch 16/59
----------
train Loss: 0.0285 Acc: 0.9992
val Loss: 0.0119 Acc: 0.9374
Training complete in 38m 2s

Epoch 17/59
----------
train Loss: 0.0285 Acc: 0.9993
val Loss: 0.0109 Acc: 0.9374
Training complete in 40m 12s

Epoch 18/59
----------
train Loss: 0.0264 Acc: 0.9993
val Loss: 0.0117 Acc: 0.9374
Training complete in 42m 21s

Epoch 19/59
----------
train Loss: 0.0265 Acc: 0.9994
val Loss: 0.0113 Acc: 0.9374
Training complete in 44m 31s

Epoch 20/59
----------
train Loss: 0.0274 Acc: 0.9994
val Loss: 0.0118 Acc: 0.9374
Training complete in 46m 42s

Epoch 21/59
----------
train Loss: 0.0269 Acc: 0.9994
val Loss: 0.0119 Acc: 0.9374
Training complete in 48m 57s

Epoch 22/59
----------
train Loss: 0.0263 Acc: 0.9994
val Loss: 0.0114 Acc: 0.9374
Training complete in 51m 8s

Epoch 23/59
----------
train Loss: 0.0258 Acc: 0.9994
val Loss: 0.0113 Acc: 0.9374
Training complete in 53m 20s

Epoch 24/59
----------
train Loss: 0.0249 Acc: 0.9994
val Loss: 0.0111 Acc: 0.9374
Training complete in 55m 30s

Epoch 25/59
----------
train Loss: 0.0260 Acc: 0.9994
val Loss: 0.0111 Acc: 0.9374
Training complete in 57m 43s

Epoch 26/59
----------
train Loss: 0.0250 Acc: 0.9994
val Loss: 0.0115 Acc: 0.9374
Training complete in 59m 55s

Epoch 27/59
----------
train Loss: 0.0247 Acc: 0.9994
val Loss: 0.0109 Acc: 0.9374
Training complete in 62m 7s

Epoch 28/59
----------
train Loss: 0.0252 Acc: 0.9994
val Loss: 0.0111 Acc: 0.9374
Training complete in 64m 26s

Epoch 29/59
----------
train Loss: 0.0246 Acc: 0.9994
val Loss: 0.0110 Acc: 0.9374
Training complete in 66m 42s

Epoch 30/59
----------
train Loss: 0.0246 Acc: 0.9994
val Loss: 0.0112 Acc: 0.9374
Training complete in 68m 53s

Epoch 31/59
----------
train Loss: 0.0239 Acc: 0.9994
val Loss: 0.0103 Acc: 0.9374
Training complete in 71m 11s

Epoch 32/59
----------
train Loss: 0.0246 Acc: 0.9994
val Loss: 0.0103 Acc: 0.9374
Training complete in 73m 33s

Epoch 33/59
----------
train Loss: 0.0242 Acc: 0.9994
val Loss: 0.0108 Acc: 0.9374
Training complete in 75m 57s

Epoch 34/59
----------
train Loss: 0.0237 Acc: 0.9994
val Loss: 0.0101 Acc: 0.9374
Training complete in 78m 11s

Epoch 35/59
----------
train Loss: 0.0235 Acc: 0.9994
val Loss: 0.0109 Acc: 0.9374
Training complete in 80m 28s

Epoch 36/59
----------
train Loss: 0.0237 Acc: 0.9994
val Loss: 0.0101 Acc: 0.9374
Training complete in 82m 46s

Epoch 37/59
----------
train Loss: 0.0235 Acc: 0.9994
val Loss: 0.0107 Acc: 0.9374
Training complete in 84m 57s

Epoch 38/59
----------
train Loss: 0.0234 Acc: 0.9994
val Loss: 0.0095 Acc: 0.9374
Training complete in 87m 6s

Epoch 39/59
----------
train Loss: 0.0233 Acc: 0.9994
val Loss: 0.0096 Acc: 0.9374
Training complete in 89m 16s

Epoch 40/59
----------
train Loss: 0.0206 Acc: 0.9994
val Loss: 0.0088 Acc: 0.9374
Training complete in 91m 27s

Epoch 41/59
----------
train Loss: 0.0193 Acc: 0.9994
val Loss: 0.0085 Acc: 0.9374
Training complete in 93m 36s

Epoch 42/59
----------
train Loss: 0.0187 Acc: 0.9994
val Loss: 0.0081 Acc: 0.9374
Training complete in 95m 46s

Epoch 43/59
----------
train Loss: 0.0186 Acc: 0.9994
val Loss: 0.0085 Acc: 0.9374
Training complete in 97m 56s

Epoch 44/59
----------
train Loss: 0.0185 Acc: 0.9994
val Loss: 0.0079 Acc: 0.9374
Training complete in 100m 6s

Epoch 45/59
----------
train Loss: 0.0190 Acc: 0.9994
val Loss: 0.0087 Acc: 0.9374
Training complete in 102m 16s

Epoch 46/59
----------
train Loss: 0.0188 Acc: 0.9994
val Loss: 0.0086 Acc: 0.9374
Training complete in 104m 25s

Epoch 47/59
----------
train Loss: 0.0190 Acc: 0.9994
val Loss: 0.0091 Acc: 0.9374
Training complete in 106m 35s

Epoch 48/59
----------
train Loss: 0.0188 Acc: 0.9994
val Loss: 0.0088 Acc: 0.9374
Training complete in 108m 46s

Epoch 49/59
----------
train Loss: 0.0193 Acc: 0.9994
val Loss: 0.0088 Acc: 0.9374
Training complete in 110m 56s

Epoch 50/59
----------
train Loss: 0.0194 Acc: 0.9994
val Loss: 0.0086 Acc: 0.9374
Training complete in 113m 6s

Epoch 51/59
----------
train Loss: 0.0195 Acc: 0.9994
val Loss: 0.0087 Acc: 0.9374
Training complete in 115m 16s

Epoch 52/59
----------
train Loss: 0.0191 Acc: 0.9994
val Loss: 0.0086 Acc: 0.9374
Training complete in 117m 26s

Epoch 53/59
----------
train Loss: 0.0193 Acc: 0.9994
val Loss: 0.0087 Acc: 0.9374
Training complete in 119m 37s

Epoch 54/59
----------
train Loss: 0.0191 Acc: 0.9994
val Loss: 0.0092 Acc: 0.9374
Training complete in 121m 47s

Epoch 55/59
----------
train Loss: 0.0196 Acc: 0.9994
val Loss: 0.0086 Acc: 0.9374
Training complete in 123m 57s

Epoch 56/59
----------
train Loss: 0.0196 Acc: 0.9994
val Loss: 0.0086 Acc: 0.9374
Training complete in 126m 8s

Epoch 57/59
----------
train Loss: 0.0200 Acc: 0.9994
val Loss: 0.0092 Acc: 0.9374
Training complete in 128m 19s

Epoch 58/59
----------
train Loss: 0.0197 Acc: 0.9994
val Loss: 0.0088 Acc: 0.9374
Training complete in 130m 30s

Epoch 59/59
----------
train Loss: 0.0202 Acc: 0.9994
val Loss: 0.0091 Acc: 0.9374
Training complete in 132m 41s

Training complete in 132m 41s
This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0
-------test-----------
256
512
768
1024
1280
1536
1792
2048
2304
2560
2816
3072
3328
3584
3840
4096
4352
4608
4864
5120
5376
5632
5888
6144
6400
6656
6912
7168
7424
7680
7936
8192
8448
8704
8960
9216
9472
9728
9984
10240
10496
10752
11008
11264
11520
11776
12032
12288
12544
12800
13056
13312
13568
13824
14080
14336
14592
14848
15104
15360
15616
15872
16128
16384
16640
16896
17152
17408
17664
17920
18176
18432
18688
18944
19200
19456
19712
19732
256
512
768
1024
1280
1536
1792
2048
2304
2560
2816
3072
3328
3368
torch.Size([3368, 512])
Rank@1:0.878266 Rank@5:0.952197 Rank@10:0.969418 mAP:0.702516
Reranking complete in 1m 17s
top1:0.897862 top5:0.943290 top10:0.962292 mAP:0.844396
